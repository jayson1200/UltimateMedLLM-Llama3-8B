wandb_version: 1

_wandb:
  desc: null
  value:
    python_version: 3.9.19
    cli_version: 0.17.0
    framework: torch
    is_jupyter_run: false
    is_kaggle_kernel: false
    start_time: 1717327476
    t:
      1:
      - 1
      - 49
      - 51
      - 55
      2:
      - 1
      - 49
      - 51
      - 55
      3:
      - 2
      - 3
      - 7
      - 23
      - 66
      4: 3.9.19
      5: 0.17.0
      8:
      - 5
      9:
        2: torchtune
      13: linux-x86_64
    m:
    - 1: total_training_steps
      6:
      - 3
    - 1: loss
      5: 1
      6:
      - 1
    - 1: lr
      5: 1
      6:
      - 1
    - 1: gpu_resources
      5: 1
      6:
      - 1
    - 1: peak_memory_active
      5: 1
      6:
      - 1
    - 1: peak_memory_alloc
      5: 1
      6:
      - 1
    - 1: peak_memory_reserved
      5: 1
      6:
      - 1
model:
  desc: null
  value:
    _component_: torchtune.models.llama3.lora_llama3_8b
    lora_attn_modules:
    - q_proj
    - v_proj
    apply_lora_to_mlp: false
    apply_lora_to_output: false
    lora_rank: 8
    lora_alpha: 16
tokenizer:
  desc: null
  value:
    _component_: torchtune.models.llama3.llama3_tokenizer
    path: /home/meribejayson/Desktop/Projects/UltimateMedLLM-Llama3-8B/training_llama_3/.checkpoints/original/tokenizer.model
checkpointer:
  desc: null
  value:
    _component_: torchtune.utils.FullModelMetaCheckpointer
    checkpoint_dir: /home/meribejayson/Desktop/Projects/UltimateMedLLM-Llama3-8B/training_llama_3/.checkpoints/original
    checkpoint_files:
    - consolidated.00.pth
    recipe_checkpoint: null
    output_dir: /home/meribejayson/Desktop/Projects/UltimateMedLLM-Llama3-8B/training_llama_3/.checkpoints/llama3
    model_type: LLAMA3
resume_from_checkpoint:
  desc: null
  value: false
dataset:
  desc: null
  value:
    _component_: torchtune.datasets.instruct_dataset
    source: szhang120/unified_large_medical_llama3_8b_instruct_dataset
    train_on_input: true
    template: MedLLMTemplate
    max_seq_len: 1024
    split: train
seed:
  desc: null
  value: null
shuffle:
  desc: null
  value: true
batch_size:
  desc: null
  value: 2
optimizer:
  desc: null
  value:
    _component_: torch.optim.AdamW
    weight_decay: 0.01
    lr: 0.0003
lr_scheduler:
  desc: null
  value:
    _component_: torchtune.modules.get_cosine_schedule_with_warmup
    num_warmup_steps: 100
loss:
  desc: null
  value:
    _component_: torch.nn.CrossEntropyLoss
epochs:
  desc: null
  value: 4
max_steps_per_epoch:
  desc: null
  value: null
gradient_accumulation_steps:
  desc: null
  value: 64
compile:
  desc: null
  value: false
output_dir:
  desc: null
  value: /home/meribejayson/Desktop/Projects/UltimateMedLLM-Llama3-8B/training_llama_3/lora_finetune_out
metric_logger:
  desc: null
  value:
    _component_: torchtune.utils.metric_logging.WandBLogger
    project: CS224n
log_every_n_steps:
  desc: null
  value: null
device:
  desc: null
  value: cuda
dtype:
  desc: null
  value: bf16
enable_activation_checkpointing:
  desc: null
  value: true
profiler:
  desc: null
  value:
    _component_: torchtune.utils.profiler
    enabled: false
