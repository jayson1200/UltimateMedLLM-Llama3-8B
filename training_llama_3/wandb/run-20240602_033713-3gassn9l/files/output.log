INFO:torchtune.utils.logging:Logging /home/meribejayson/Desktop/Projects/UltimateMedLLM-Llama3-8B/training_llama_3/.checkpoints/original/torchtune_config.yaml to W&B under Files
INFO:torchtune.utils.logging:Model is initialized with precision torch.bfloat16.
INFO:torchtune.utils.logging:Memory Stats after model init:
{'peak_memory_active': 16.572998144, 'peak_memory_alloc': 16.572998144, 'peak_memory_reserved': 16.642998272}
INFO:torchtune.utils.logging:Tokenizer is initialized from file.
INFO:torchtune.utils.logging:Optimizer and loss are initialized.
INFO:torchtune.utils.logging:Loss is initialized.
Downloading data: 100%|████████████████████████████████████████████████████████████████████████████████| 9.99M/9.99M [00:00<00:00, 23.2MB/s]
Generating train split: 100%|██████████████████████████████████████████████████████████████| 12660/12660 [00:00<00:00, 349180.57 examples/s]
INFO:torchtune.utils.logging:Dataset and Sampler are initialized.
INFO:torchtune.utils.logging:Learning rate scheduler is initialized.

1|1|Loss: 2.0842018127441406: 100%|███████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  1.43it/s]
INFO:torchtune.utils.logging:Model checkpoint of size 16.06 GB saved to /home/meribejayson/Desktop/Projects/UltimateMedLLM-Llama3-8B/training_llama_3/.checkpoints/llama3/meta_model_0.pt
INFO:torchtune.utils.logging:Adapter checkpoint of size 0.01 GB saved to /home/meribejayson/Desktop/Projects/UltimateMedLLM-Llama3-8B/training_llama_3/.checkpoints/llama3/adapter_0.pt