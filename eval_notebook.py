# -*- coding: utf-8 -*-
"""Another copy of llama_3_8b_instruct_medical.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lGO5A1kGoacD9j6KODfC3t0-lJdmITCi
"""

# Installing all the necessary packages(libraries) for the experiments (RTX30xx, RTX40xx, A100, H100, L40)
!pip install --no-deps packaging ninja einops flash-attn xformers trl peft accelerate bitsandbytes
!pip install "unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git"
print('dog')

# Importing the necessary packages(libraries) for the experiments
import json

import torch
from datasets import load_dataset
from huggingface_hub import notebook_login
from transformers import TrainingArguments
from trl import SFTTrainer
from unsloth import FastLanguageModel

# Logging into the Hugging Face Hub(with token)
notebook_login()

import torch
from unsloth import FastLanguageModel
from transformers import AutoTokenizer

# Define the configuration for the model and LoRA
config = {
    "hugging_face_username": "szhang120",
    "model_config": {
        "base_model": "unsloth/llama-3-8b-Instruct-bnb-4bit",  # The base model
        "finetuned_model": "szhang120/llama-3-8b-Instruct-bnb-4bit-medical",  # The finetuned model name
        "max_seq_length": 2048,  # The maximum sequence length
        "dtype": torch.float16,  # The data type
        "load_in_4bit": True,  # Load the model in 4-bit
    },
    "lora_config": {
        "r": 16,  # The number of LoRA layers
        "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj",
                           "gate_proj", "up_proj", "down_proj"],  # The target modules
        "lora_alpha": 16,  # The alpha value for LoRA
        "lora_dropout": 0,  # The dropout value for LoRA
        "bias": "none",  # The bias for LoRA
        "use_gradient_checkpointing": True,  # Use gradient checkpointing
        "use_rslora": False,  # Use RSLora
        "use_dora": False,  # Use DoRa
        "loftq_config": None  # The LoFTQ configuration
    },
    "training_dataset": {
        "name": "szhang120/medical_llama3_instruct_dataset_short",  # The dataset name (Hugging Face Datasets)
        "split": "train",  # The dataset split
        "input_field": "prompt",  # The input field
    },
    "training_config": {
        "per_device_train_batch_size": 2,  # The batch size
        "gradient_accumulation_steps": 4,  # The gradient accumulation steps
        "warmup_steps": 5,  # The warmup steps
        "max_steps": 0,  # The maximum steps (0 if the epochs are defined)
        "num_train_epochs": 1,  # The number of training epochs (0 if the maximum steps are defined)
        "learning_rate": 2e-4,  # The learning rate
        "fp16": not torch.cuda.is_bf16_supported(),  # Use FP16 if BF16 is not supported
        "bf16": torch.cuda.is_bf16_supported(),  # Use BF16 if supported
        "logging_steps": 1,  # The logging steps
        "optim": "adamw_8bit",  # The optimizer
        "weight_decay": 0.01,  # The weight decay
        "lr_scheduler_type": "linear",  # The learning rate scheduler
        "seed": 42,  # The seed
        "output_dir": "outputs",  # The output directory
    }
}

# Load the fine-tuned model and the tokenizer
model = FastLanguageModel.from_pretrained(
    config["model_config"]["finetuned_model"],
    max_seq_length=config["model_config"]["max_seq_length"],
    dtype=config["model_config"]["dtype"],
    load_in_4bit=config["model_config"]["load_in_4bit"]
)[0]  # Ensure we get the model, not a tuple

print(model.config)

tokenizer = AutoTokenizer.from_pretrained(config["model_config"]["finetuned_model"])

# Use FastLanguageModel for fast inference
FastLanguageModel.for_inference(model)

from google.colab import files
uploaded = files.upload()

import json
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from tqdm import tqdm

# Function to generate responses from the model in batches
def get_model_responses(model, tokenizer, questions, options_list, batch_size=8):
    prompts = [
        f"Question: {question}\nOptions:\n" + "\n".join([f"{key}: {value}" for key, value in options.items()]) + "\nAnswer:"
        for question, options in zip(questions, options_list)
    ]
    inputs = tokenizer(prompts, return_tensors="pt", padding=True, truncation=True)
    input_ids = inputs['input_ids'].to(model.device)
    attention_mask = inputs['attention_mask'].to(model.device)

    outputs = model.generate(input_ids, attention_mask=attention_mask, max_new_tokens=10, pad_token_id=tokenizer.eos_token_id)
    answers = [tokenizer.decode(output, skip_special_tokens=True).split("Answer:")[-1].strip().split("\n")[0] for output in outputs]
    return answers

# Function to determine the accuracy and print responses
def evaluate_model(model, tokenizer, test_data, batch_size=8):
    correct_predictions = 0
    total_predictions = 0

    for i in tqdm(range(0, len(test_data), batch_size)):
        batch = test_data[i:i+batch_size]
        questions = [entry["question"] for entry in batch]
        options_list = [entry["options"] for entry in batch]
        correct_answers = [entry["answer_idx"] for entry in batch]

        # Get the model's predictions
        predictions = get_model_responses(model, tokenizer, questions, options_list, batch_size)

        # Check if the predictions are correct
        for question, options, correct_ans, pred in zip(questions, options_list, correct_answers, predictions):
            total_predictions += 1
            predicted_answer = None
            for key, value in options.items():
                if value in pred:
                    predicted_answer = key
                    break
            predicted_answer = pred[0] if pred else "None"
            if predicted_answer == correct_ans:
                correct_predictions += 1
            else:
                print(f"Question: {question}")
                print("Options:")
                for key, value in options.items():
                    print(f"  {key}: {value}")
                print(f"Correct Answer: {correct_ans}")
                print(f"Model's Response: {predicted_answer}")
                print()

    # Calculate accuracy
    accuracy = correct_predictions / total_predictions
    return accuracy

# Load your test dataset
test_data_path = "/content/test.jsonl"  # Update this path based on your uploaded file
with open(test_data_path, "r") as f:
    test_data = [json.loads(line) for line in f]

# Evaluate the model
accuracy = evaluate_model(model, tokenizer, test_data)
print(f"Model Accuracy: {accuracy * 100:.2f}%")