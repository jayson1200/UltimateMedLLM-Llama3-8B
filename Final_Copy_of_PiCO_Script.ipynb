{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NEJkyqw8IvPJ"
      },
      "outputs": [],
      "source": [
        "# 1. Clone PiCO Github Repo files onto Colab Session\n",
        "\n",
        "!git clone https://github.com/PKU-YuanGroup/Peer-review-in-LLMs.git\n",
        "%cd Peer-review-in-LLMs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CxWjNXZlIzkU"
      },
      "outputs": [],
      "source": [
        "# 2. Install requirements.txt, especially for the '\"vllm\" thing.\n",
        "!pip install --upgrade pip\n",
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ZR-GPtCLEj5"
      },
      "outputs": [],
      "source": [
        "''' 3. Data 1:\n",
        "Upload your zipped mt_bench2, containing 'questions.jsonl' (adversarial) and 'questions3' (old)\n",
        "as set up before like in our local folder. Next we'll actually get 'questions.jsonl' where it has to go.\n",
        "'''\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp '/content/drive/My Drive/question.jsonl' data/"
      ],
      "metadata": {
        "id": "h87NIJAeMf9M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MODEL ANSWER GENERATION\n",
        "# FOR EACH CLOSED-SOURCE PROVIDER, SET API KEY SECRET.\n",
        "\n",
        "import os\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = ''"
      ],
      "metadata": {
        "id": "7WMaUM9ka47k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "StJzq1VfJNAU"
      },
      "outputs": [],
      "source": [
        "# Model Answer Generation (Proprietary)\n",
        "\n",
        "# List of models for answer generation\n",
        "closed_models = [\n",
        "    {'model_path': 'openai/gpt-3.5-turbo', 'model_id': 'gpt-3.5-turbo'}\n",
        "]\n",
        "\n",
        "# Loop through each model and generate responses\n",
        "for model in closed_models:\n",
        "    !python llm_judge/gen_model_answer.py \\\n",
        "        --model-path {model['model_path']} \\\n",
        "        --model-id {model['model_id']} \\\n",
        "        --bench-name mt_bench"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Answer Generation (Open)\n",
        "\n",
        "open_models = [\n",
        "    {'model_path': 'lmsys/vicuna-7b-v1.5', 'model_id': 'vicuna-7b-v1.5'},\n",
        "    {'model_path': 'microsoft/Phi-3-mini-4k-instruct', 'model_id': 'Phi-3-mini-4k-instruct'},\n",
        "    {'model_path': 'lmsys/fastchat-t5-3b-v1.0', 'model_id': 'fastchat-t5-3b-v1.0'},\n",
        "    {'model_path': 'THUDM/chatglm-6b', 'model_id': 'chatglm-6b'}\n",
        "]\n",
        "\n",
        "for model in open_models:\n",
        "    !python llm_judge/gen_model_answer.py \\\n",
        "        --model-path {model['model_path']} \\\n",
        "        --model-id {model['model_id']} \\\n",
        "        --bench-name mt_bench"
      ],
      "metadata": {
        "id": "MNyYXeU1y_HS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kt6qbvL6JP6z"
      },
      "outputs": [],
      "source": [
        "# After answers generated\n",
        "\n",
        "!python llm_judge/assign_judge.py"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mv llm_judge/data/judge_prompts.jsonl data/"
      ],
      "metadata": {
        "id": "WGGV77Ad21Fl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tvba0_ImJRmi"
      },
      "outputs": [],
      "source": [
        "# Judgment of Pairs; Specify (real, existing on HuggingFace) Judge Model Name or Names.\n",
        "# Error: judge model smth smth\n",
        "# Error: no judge_prompts file in Data\n",
        "# Error:\n",
        "\n",
        "!export OPENAI_API_KEY=''\n",
        "!python llm_judge/gen_judgment.py \\\n",
        "--mode pairwise-all \\\n",
        "--new-judge-model gpt-3.5-turbo \\\n",
        "--model-list fastchat-t5-3b-v1.0 vicuna-7b-v1.5 gpt-3.5-turbo \\\n",
        "--batch-size 512 \\\n",
        "--bench-name mt_bench\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l-sM8rbxJT6E"
      },
      "outputs": [],
      "source": [
        "# Ablation\n",
        "!ls\n",
        "!ls data/mt_bench/model_judgment\n",
        "!cd con_optimization && python main_ablation.py --baseline 0 --mode Order --epoch 30"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r /content/final_project_int.zip /content/Peer-review-in-LLMs"
      ],
      "metadata": {
        "id": "51uNZg7ATmyZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('/content/final_project.zip')"
      ],
      "metadata": {
        "id": "RAlMYqI6WrFK"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}